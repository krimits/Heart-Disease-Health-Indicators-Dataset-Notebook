{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Cancer Prediction - Optimized ML Pipeline\n",
    "\n",
    "## Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ Notebook Î³Î¹Î± Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÎšÎ±ÏÎºÎ¯Î½Î¿Ï…\n",
    "\n",
    "**Dataset**: Wisconsin Breast Cancer Dataset (569 samples, 30 features)\n",
    "\n",
    "**Î£Ï„ÏŒÏ‡Î¿Ï‚**: Binary Classification - Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ· (M=Malignant/ÎšÎ±ÎºÎ¿Î®Î¸Î·Ï‚, B=Benign/ÎšÎ±Î»Î¿Î®Î¸Î·Ï‚)\n",
    "\n",
    "**Î’ÎµÎ»Ï„Î¹ÏŽÏƒÎµÎ¹Ï‚**:\n",
    "- âœ… Fixed data leakage issue\n",
    "- âœ… Cross-validation implementation\n",
    "- âœ… Multiple ML models comparison\n",
    "- âœ… Hyperparameter tuning\n",
    "- âœ… Feature selection & importance analysis\n",
    "- âœ… Enhanced visualizations (ROC, confusion matrices, etc.)\n",
    "- âœ… Model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Machine Learning - Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Model Persistence\n",
    "import joblib\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "url = 'https://raw.githubusercontent.com/ybifoundation/Dataset/main/Cancer.csv'\n",
    "cancer = pd.read_csv(url)\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully!\")\n",
    "print(f\"Shape: {cancer.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 3: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"ðŸ“‹ Dataset Information:\")\n",
    "print(cancer.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nðŸ“Š Statistical Summary:\")\n",
    "print(cancer.describe())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nâ“ Missing Values:\")\n",
    "missing = cancer.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Target distribution\n",
    "print(\"\\nðŸŽ¯ Target Distribution:\")\n",
    "print(cancer['diagnosis'].value_counts())\n",
    "print(f\"\\nClass Balance: {cancer['diagnosis'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "cancer['diagnosis'].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Diagnosis Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Diagnosis (B=Benign, M=Malignant)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Benign', 'Malignant'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "cancer['diagnosis'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                         colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "axes[1].set_title('Diagnosis Proportion', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Step 4: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "columns_to_drop = ['id', 'Unnamed: 32'] if 'Unnamed: 32' in cancer.columns else ['id']\n",
    "cancer_clean = cancer.drop(columns=columns_to_drop)\n",
    "\n",
    "# Define features and target\n",
    "X = cancer_clean.drop('diagnosis', axis=1)\n",
    "y = cancer_clean['diagnosis']\n",
    "\n",
    "print(f\"âœ… Features shape: {X.shape}\")\n",
    "print(f\"âœ… Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for top features\n",
    "plt.figure(figsize=(16, 14))\n",
    "correlation_matrix = X.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr = np.where(np.abs(correlation_matrix) > 0.9)\n",
    "high_corr_list = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y]) \n",
    "                  for x, y in zip(*high_corr) if x != y and x < y]\n",
    "print(f\"\\nðŸ”— Highly Correlated Features (>0.9): {len(high_corr_list)} pairs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”€ Step 5: Train-Test Split (FIXED - No Data Leakage!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data BEFORE any scaling or preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train set: {X_train.shape} | Test set: {X_test.shape}\")\n",
    "print(f\"\\nTrain class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTest class distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ Step 6: Feature Scaling (CORRECTED - After Split!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FIT on training data ONLY, then transform both\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Only transform, no fit!\n",
    "\n",
    "# Convert back to DataFrame for better readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"âœ… Feature scaling completed correctly (NO data leakage!)\")\n",
    "print(f\"\\nScaled training data sample:\")\n",
    "print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 7: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: SelectKBest (ANOVA F-test)\n",
    "selector = SelectKBest(score_func=f_classif, k=15)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Score': selector.scores_\n",
    "}).sort_values('Score', ascending=False)\n",
    "\n",
    "print(f\"âœ… Top 15 features selected:\")\n",
    "print(selected_features)\n",
    "print(f\"\\nðŸ“Š Feature Importance Scores:\")\n",
    "print(feature_scores.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_scores.head(15)\n",
    "sns.barplot(data=top_features, y='Feature', x='Score', palette='viridis')\n",
    "plt.title('Top 15 Most Important Features (ANOVA F-Score)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('F-Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 8: Baseline Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"ðŸ”„ Training and evaluating models with 5-fold cross-validation...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label='M')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='M')\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='M')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… {name}:\")\n",
    "    print(f\"   CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"   Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\\n\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š Model Comparison Summary:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Test Accuracy\n",
    "sns.barplot(data=results_df, x='Test Accuracy', y='Model', ax=axes[0, 0], palette='Blues_r')\n",
    "axes[0, 0].set_title('Test Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlim(0.9, 1.0)\n",
    "\n",
    "# Cross-Validation Mean\n",
    "sns.barplot(data=results_df, x='CV Mean', y='Model', ax=axes[0, 1], palette='Greens_r')\n",
    "axes[0, 1].set_title('Cross-Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlim(0.9, 1.0)\n",
    "\n",
    "# Precision\n",
    "sns.barplot(data=results_df, x='Precision', y='Model', ax=axes[1, 0], palette='Oranges_r')\n",
    "axes[1, 0].set_title('Precision Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlim(0.9, 1.0)\n",
    "\n",
    "# F1-Score\n",
    "sns.barplot(data=results_df, x='F1-Score', y='Model', ax=axes[1, 1], palette='Reds_r')\n",
    "axes[1, 1].set_title('F1-Score Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlim(0.9, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 9: Hyperparameter Tuning (Best Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 3 models for hyperparameter tuning\n",
    "top_3_models = results_df.head(3)['Model'].tolist()\n",
    "print(f\"ðŸŽ¯ Tuning hyperparameters for top 3 models: {top_3_models}\\n\")\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'SVM (RBF)': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]\n",
    "    },\n",
    "    'SVM (Linear)': {\n",
    "        'C': [0.1, 1, 10, 100]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune top models\n",
    "tuned_models = {}\n",
    "\n",
    "for name in top_3_models:\n",
    "    if name in param_grids:\n",
    "        print(f\"\\nðŸ”§ Tuning {name}...\")\n",
    "        \n",
    "        model = models[name]\n",
    "        param_grid = param_grids[name]\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grid, cv=cv, scoring='accuracy', \n",
    "            n_jobs=-1, verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        tuned_models[name] = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"   âœ… Best params: {grid_search.best_params_}\")\n",
    "        print(f\"   âœ… Best CV score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"   âœ… Test accuracy: {grid_search.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ† Step 10: Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned models\n",
    "best_score = 0\n",
    "best_model_name = None\n",
    "best_model = None\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    score = model.score(X_test_scaled, y_test)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_name = name\n",
    "        best_model = model\n",
    "\n",
    "print(f\"ðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"ðŸŽ¯ Test Accuracy: {best_score:.4f} ({best_score*100:.2f}%)\")\n",
    "print(f\"\\nðŸ“‹ Model Parameters:\")\n",
    "print(best_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 11: Detailed Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['B', 'M'])\n",
    "\n",
    "# Classification Report\n",
    "print(\"ðŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malignant']))\n",
    "\n",
    "# Detailed metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='M')\n",
    "recall = recall_score(y_test, y_pred, pos_label='M')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='M')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"   Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "\n",
    "if y_pred_proba is not None:\n",
    "    auc_score = roc_auc_score(y_test.map({'B': 0, 'M': 1}), y_pred_proba)\n",
    "    print(f\"   AUC-ROC:   {auc_score:.4f} ({auc_score*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}\\n(Counts)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Normalized Confusion Matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
    "axes[1].set_title(f'Confusion Matrix - {best_model_name}\\n(Normalized)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix interpretation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nðŸ” Confusion Matrix Breakdown:\")\n",
    "print(f\"   True Negatives (Benign correctly identified):  {tn}\")\n",
    "print(f\"   False Positives (Benign predicted as Malignant): {fp}\")\n",
    "print(f\"   False Negatives (Malignant predicted as Benign): {fn}\")\n",
    "print(f\"   True Positives (Malignant correctly identified): {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve and Precision-Recall Curve\n",
    "if y_pred_proba is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # ROC Curve\n",
    "    y_test_binary = y_test.map({'B': 0, 'M': 1})\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_test_binary, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_test_binary, y_pred_proba)\n",
    "    \n",
    "    axes[0].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title(f'ROC Curve - {best_model_name}', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test_binary, y_pred_proba)\n",
    "    \n",
    "    axes[1].plot(recall_curve, precision_curve, linewidth=2, label=f'{best_model_name}')\n",
    "    axes[1].axhline(y=y_test_binary.mean(), color='k', linestyle='--', linewidth=1, label='Baseline')\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].set_title(f'Precision-Recall Curve - {best_model_name}', fontsize=12, fontweight='bold')\n",
    "    axes[1].legend(loc='lower left')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Step 12: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"ðŸ“Š Top 15 Most Important Features:\")\n",
    "    print(feature_importance.head(15))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=feature_importance.head(15), y='Feature', x='Importance', palette='rocket')\n",
    "    plt.title(f'Top 15 Feature Importances - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models (Logistic Regression, SVM Linear)\n",
    "    coef = best_model.coef_[0] if best_model.coef_.ndim > 1 else best_model.coef_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': coef,\n",
    "        'Abs_Coefficient': np.abs(coef)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"ðŸ“Š Top 15 Most Important Features (by coefficient magnitude):\")\n",
    "    print(feature_importance.head(15))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_15 = feature_importance.head(15)\n",
    "    colors = ['red' if x < 0 else 'green' for x in top_15['Coefficient']]\n",
    "    sns.barplot(data=top_15, y='Feature', x='Coefficient', palette=colors)\n",
    "    plt.title(f'Top 15 Feature Coefficients - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ Feature importance not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 13: Save Model & Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_filename = 'best_cancer_model.pkl'\n",
    "scaler_filename = 'feature_scaler.pkl'\n",
    "\n",
    "joblib.dump(best_model, model_filename)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "print(f\"âœ… Model saved as '{model_filename}'\")\n",
    "print(f\"âœ… Scaler saved as '{scaler_filename}'\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'test_accuracy': best_score,\n",
    "    'features': list(X.columns),\n",
    "    'target_classes': ['B', 'M']\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ“‹ Model Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    if key != 'features':\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 14: Example Prediction with Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (demonstration)\n",
    "loaded_model = joblib.load(model_filename)\n",
    "loaded_scaler = joblib.load(scaler_filename)\n",
    "\n",
    "print(\"âœ… Model and scaler loaded successfully!\")\n",
    "\n",
    "# Make prediction on a sample\n",
    "sample = X_test.iloc[0:1]\n",
    "sample_scaled = loaded_scaler.transform(sample)\n",
    "prediction = loaded_model.predict(sample_scaled)\n",
    "prediction_proba = loaded_model.predict_proba(sample_scaled) if hasattr(loaded_model, 'predict_proba') else None\n",
    "\n",
    "print(f\"\\nðŸ”® Sample Prediction:\")\n",
    "print(f\"   True Label: {y_test.iloc[0]}\")\n",
    "print(f\"   Predicted Label: {prediction[0]}\")\n",
    "if prediction_proba is not None:\n",
    "    print(f\"   Prediction Probability: Benign={prediction_proba[0][0]:.4f}, Malignant={prediction_proba[0][1]:.4f}\")\n",
    "print(f\"   Result: {'âœ… Correct' if prediction[0] == y_test.iloc[0] else 'âŒ Incorrect'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Step 15: Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ¯ CANCER PREDICTION - OPTIMIZED ML PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Dataset Information:\")\n",
    "print(f\"   Total Samples: {len(cancer)}\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Train/Test Split: {len(X_train)}/{len(X_test)}\")\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {best_score:.4f} ({best_score*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall: {recall:.4f}\")\n",
    "print(f\"   F1-Score: {f1:.4f}\")\n",
    "if y_pred_proba is not None:\n",
    "    print(f\"   AUC-ROC: {auc_score:.4f}\")\n",
    "print(f\"\\nâœ… Key Improvements Implemented:\")\n",
    "print(f\"   âœ“ Fixed data leakage (scaling after train-test split)\")\n",
    "print(f\"   âœ“ Cross-validation (5-fold stratified)\")\n",
    "print(f\"   âœ“ Multiple model comparison ({len(models)} models tested)\")\n",
    "print(f\"   âœ“ Hyperparameter tuning (GridSearchCV)\")\n",
    "print(f\"   âœ“ Feature selection & importance analysis\")\n",
    "print(f\"   âœ“ Comprehensive visualizations (ROC, PR curves, confusion matrices)\")\n",
    "print(f\"   âœ“ Model persistence (saved for future use)\")\n",
    "print(f\"\\nðŸ’¾ Saved Files:\")\n",
    "print(f\"   - {model_filename}\")\n",
    "print(f\"   - {scaler_filename}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Pipeline completed successfully!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
