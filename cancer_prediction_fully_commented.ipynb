{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Cancer Prediction - Î Î»Î®ÏÏ‰Ï‚ Î£Ï‡Î¿Î»Î¹Î±ÏƒÎ¼Î­Î½Î· ÎˆÎºÎ´Î¿ÏƒÎ·\n",
    "\n",
    "## Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ ML Pipeline Î¼Îµ Î‘Î½Î±Î»Ï…Ï„Î¹ÎºÎ¬ Î£Ï‡ÏŒÎ»Î¹Î±\n",
    "\n",
    "**Dataset**: Wisconsin Breast Cancer Dataset (569 samples, 30 features)\n",
    "\n",
    "**Î£Ï„ÏŒÏ‡Î¿Ï‚**: Binary Classification - Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ· (M=Malignant/ÎšÎ±ÎºÎ¿Î®Î¸Î·Ï‚, B=Benign/ÎšÎ±Î»Î¿Î®Î¸Î·Ï‚)\n",
    "\n",
    "**Î‘Ï…Ï„Î® Î· Î­ÎºÎ´Î¿ÏƒÎ· Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹**:\n",
    "- ğŸ“ Î‘Î½Î±Î»Ï…Ï„Î¹ÎºÎ¬ ÏƒÏ‡ÏŒÎ»Î¹Î± ÏƒÎµ ÎºÎ¬Î¸Îµ Î³ÏÎ±Î¼Î¼Î® ÎºÏÎ´Î¹ÎºÎ±\n",
    "- ğŸ’¡ Î•Ï€ÎµÎ¾Î·Î³Î®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± ÎºÎ¬Î¸Îµ Î±Ï€ÏŒÏ†Î±ÏƒÎ·\n",
    "- âš ï¸ Î ÏÎ¿ÎµÎ¹Î´Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± ÏƒÏ…Î½Î·Î¸Î¹ÏƒÎ¼Î­Î½Î± Î»Î¬Î¸Î·\n",
    "- ğŸ“š Best practices ÎºÎ±Î¹ ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ­Ï‚ Ï€ÏÎ¿ÏƒÎµÎ³Î³Î¯ÏƒÎµÎ¹Ï‚\n",
    "- ğŸ“ Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÏŒ Ï…Î»Î¹ÎºÏŒ Î³Î¹Î± beginners\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 1: Import Libraries\n",
    "\n",
    "Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Ï‰Î½ Î²Î¹Î²Î»Î¹Î¿Î¸Î·ÎºÏÎ½ Î³Î¹Î±:\n",
    "- Data manipulation (pandas, numpy)\n",
    "- Visualization (matplotlib, seaborn)\n",
    "- Machine Learning (scikit-learn)\n",
    "- Model persistence (joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA MANIPULATION LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "# pandas: Î“Î¹Î± Ï‡ÎµÎ¹ÏÎ¹ÏƒÎ¼ÏŒ structured data (DataFrames/Series)\n",
    "# Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Î³Î¹Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·, Î±Î½Î¬Î»Ï…ÏƒÎ· ÎºÎ±Î¹ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½\n",
    "import pandas as pd\n",
    "\n",
    "# numpy: Î“Î¹Î± Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ­Ï‚ Ï€ÏÎ¬Î¾ÎµÎ¹Ï‚ ÎºÎ±Î¹ arrays\n",
    "# Î’Î¬ÏƒÎ· Î³Î¹Î± ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ scientific computing Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "# matplotlib.pyplot: Î’Î±ÏƒÎ¹ÎºÎ® Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ· Î³Î¹Î± plots ÎºÎ±Î¹ visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn: High-level Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ· Î³Î¹Î± statistical graphics (built on matplotlib)\n",
    "# Î Î±ÏÎ­Ï‡ÎµÎ¹ Ï€Î¹Î¿ ÏŒÎ¼Î¿ÏÏ†Î± ÎºÎ±Î¹ Ï€Î¹Î¿ ÎµÏÎºÎ¿Î»Î± plots\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================================\n",
    "# MACHINE LEARNING - PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "# train_test_split: Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÏƒÎµ training ÎºÎ±Î¹ test sets\n",
    "# cross_val_score: Cross-validation Î³Î¹Î± Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½\n",
    "# GridSearchCV: Hyperparameter tuning Î¼Îµ exhaustive search\n",
    "# StratifiedKFold: K-fold cross-validation Ï€Î¿Ï… Î´Î¹Î±Ï„Î·ÏÎµÎ¯ Ï„Î¿ class distribution\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,    # Î“Î¹Î± split Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½\n",
    "    cross_val_score,     # Î“Î¹Î± cross-validation\n",
    "    GridSearchCV,        # Î“Î¹Î± hyperparameter tuning\n",
    "    StratifiedKFold      # Î“Î¹Î± stratified k-fold CV\n",
    ")\n",
    "\n",
    "# StandardScaler: ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· features (mean=0, std=1)\n",
    "# Î£Î—ÎœÎ‘ÎÎ¤Î™ÎšÎŸ: Î ÏÎ­Ï€ÎµÎ¹ Î½Î± Î³Î¯Î½ÎµÎ¹ fit Î¼ÏŒÎ½Î¿ ÏƒÏ„Î¿ training set!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SelectKBest: Feature selection Î¼Îµ statistical tests\n",
    "# f_classif: ANOVA F-test Î³Î¹Î± classification\n",
    "# RFE: Recursive Feature Elimination (ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ® Î¼Î­Î¸Î¿Î´Î¿Ï‚)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# ============================================================================\n",
    "# MACHINE LEARNING - MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# LogisticRegression: Linear model Î³Î¹Î± binary/multiclass classification\n",
    "# Î‘Ï€Î»ÏŒ, Î³ÏÎ®Î³Î¿ÏÎ¿, interpretable\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# RandomForestClassifier: Ensemble of decision trees\n",
    "# Î™ÏƒÏ‡Ï…ÏÏŒ, resistant to overfitting, Ï€Î±ÏÎ­Ï‡ÎµÎ¹ feature importance\n",
    "# GradientBoostingClassifier: Boosting algorithm, ÏƒÏ…Ï‡Î½Î¬ Ï€Î¿Î»Ï Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒ\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# SVC: Support Vector Machine classifier\n",
    "# Î‘Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒ ÏƒÎµ high-dimensional spaces, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ kernel trick\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# GaussianNB: Naive Bayes Î¼Îµ Gaussian distribution assumption\n",
    "# Î“ÏÎ®Î³Î¿ÏÎ¿, Î±Ï€Î»ÏŒ, ÎºÎ±Î»ÏŒ Î³Î¹Î± small datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# KNeighborsClassifier: Instance-based learning\n",
    "# Î¤Î±Î¾Î¹Î½Î¿Î¼ÎµÎ¯ Î²Î¬ÏƒÎµÎ¹ Ï„Ï‰Î½ k Ï€Î»Î·ÏƒÎ¹Î­ÏƒÏ„ÎµÏÏ‰Î½ Î³ÎµÎ¹Ï„ÏŒÎ½Ï‰Î½\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ============================================================================\n",
    "# MACHINE LEARNING - METRICS\n",
    "# ============================================================================\n",
    "\n",
    "# accuracy_score: Percentage of correct predictions\n",
    "# confusion_matrix: Matrix showing true/false positives/negatives\n",
    "# classification_report: Precision, recall, f1-score per class\n",
    "# roc_curve: True Positive Rate vs False Positive Rate\n",
    "# roc_auc_score: Area Under the ROC Curve (0 to 1)\n",
    "# precision_recall_curve: Precision vs Recall Î³Î¹Î± Î´Î¹Î¬Ï†Î¿ÏÎ± thresholds\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,              # Î Î¿ÏƒÎ¿ÏƒÏ„ÏŒ ÏƒÏ‰ÏƒÏ„ÏÎ½ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½\n",
    "    confusion_matrix,            # Confusion matrix\n",
    "    classification_report,       # Î‘Î½Î±Î»Ï…Ï„Î¹ÎºÏŒ report Î¼Îµ metrics\n",
    "    roc_curve,                   # Data Î³Î¹Î± ROC curve\n",
    "    roc_auc_score,              # AUC score\n",
    "    precision_recall_curve,      # Data Î³Î¹Î± Precision-Recall curve\n",
    "    f1_score,                    # F1 score (harmonic mean of precision & recall)\n",
    "    precision_score,             # Precision (TP / (TP + FP))\n",
    "    recall_score                 # Recall/Sensitivity (TP / (TP + FN))\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL PERSISTENCE\n",
    "# ============================================================================\n",
    "\n",
    "# joblib: Î“Î¹Î± Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·/Ï†ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Ï€Î¹Î¿ efficient Î±Ï€ÏŒ pickle Î³Î¹Î± numpy arrays)\n",
    "import joblib\n",
    "\n",
    "# ============================================================================\n",
    "# SETTINGS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# warnings: Î“Î¹Î± Î±Ï€ÏŒÎºÏÏ…ÏˆÎ· warnings (Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Î‘Ï€Î¿ÎºÏÏÏ€Ï„ÎµÎ¹ deprecation warnings ÎºÏ„Î»\n",
    "\n",
    "# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· seaborn style Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ± plots\n",
    "sns.set_style('whitegrid')  # Î†Î»Î»ÎµÏ‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚: 'darkgrid', 'white', 'dark', 'ticks'\n",
    "\n",
    "# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· default Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ figures\n",
    "plt.rcParams['figure.figsize'] = (12, 6)  # (width, height) ÏƒÎµ inches\n",
    "\n",
    "# Î•Ï€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ· ÏŒÏ„Î¹ ÏŒÎ»ÎµÏ‚ Î¿Î¹ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"   pandas version: {pd.__version__}\")\n",
    "print(f\"   numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 2: Load Data\n",
    "\n",
    "Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… Wisconsin Breast Cancer Dataset Î±Ï€ÏŒ GitHub.\n",
    "\n",
    "**Î£Î·Î¼ÎµÎ¹ÏÏƒÎµÎ¹Ï‚**:\n",
    "- Î¤Î¿ dataset Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ measurements Î±Ï€ÏŒ digitized images of breast mass\n",
    "- 30 features Ï€Î¿Ï… Ï€ÎµÏÎ¹Î³ÏÎ¬Ï†Î¿Ï…Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Ï„Î¿Ï… ÎºÏ…Ï„Ï„Î±ÏÎ¹ÎºÎ¿Ï Ï€Ï…ÏÎ®Î½Î±\n",
    "- Binary target: M (Malignant) Î® B (Benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Ï„Î¿Ï… dataset ÏƒÏ„Î¿ GitHub (raw format Î³Î¹Î± Î½Î± Ï†Î¿ÏÏ„ÏÏƒÎµÎ¹ ÏƒÏ‰ÏƒÏ„Î¬)\n",
    "url = 'https://raw.githubusercontent.com/ybifoundation/Dataset/main/Cancer.csv'\n",
    "\n",
    "# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… CSV file ÏƒÎµ pandas DataFrame\n",
    "# pandas Î¸Î± Î±Î½Î±Î³Î½Ï‰ÏÎ¯ÏƒÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î±:\n",
    "# - Headers (Ï€ÏÏÏ„Î· Î³ÏÎ±Î¼Î¼Î® Ï‰Ï‚ column names)\n",
    "# - Data types (Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¬, strings ÎºÏ„Î»)\n",
    "# - Missing values (NaN)\n",
    "cancer = pd.read_csv(url)\n",
    "\n",
    "# Î•Ï€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ· ÎµÏ€Î¹Ï„Ï…Ï‡Î¿ÏÏ‚ Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚\n",
    "print(f\"âœ… Dataset loaded successfully!\")\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î¿Ï… shape (rows, columns)\n",
    "# shape[0] = Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ samples, shape[1] = Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ columns\n",
    "print(f\"Shape: {cancer.shape}\")\n",
    "print(f\"   Samples (rows): {cancer.shape[0]}\")\n",
    "print(f\"   Features (columns): {cancer.shape[1]}\")\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Ï€ÏÏÏ„Ï‰Î½ 5 Î³ÏÎ±Î¼Î¼ÏÎ½ Î³Î¹Î± quick inspection\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "\n",
    "# head(n) ÎµÎ¼Ï†Î±Î½Î¯Î¶ÎµÎ¹ Ï„Î¹Ï‚ Ï€ÏÏÏ„ÎµÏ‚ n Î³ÏÎ±Î¼Î¼Î­Ï‚ (default n=5)\n",
    "# Î§ÏÎ®ÏƒÎ¹Î¼Î¿ Î³Î¹Î± Î³ÏÎ®Î³Î¿ÏÎ¿ Î­Î»ÎµÎ³Ï‡Î¿ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎºÎ±Î¹ ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï€ÏÎ¹Î½ Î±Ï€ÏŒ Î¿Ï€Î¿Î¹Î±Î´Î®Ï€Î¿Ï„Îµ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.\n",
    "\n",
    "**Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ**:\n",
    "1. Data types ÎºÎ±Î¹ memory usage\n",
    "2. Missing values\n",
    "3. Statistical summary\n",
    "4. Target distribution (class balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ“‹ Dataset Information:\")\n",
    "print()\n",
    "\n",
    "# info() Î´ÎµÎ¯Ï‡Î½ÎµÎ¹:\n",
    "# - Î‘ÏÎ¹Î¸Î¼ÏŒ entries (rows)\n",
    "# - Column names ÎºÎ±Î¹ data types\n",
    "# - Non-null counts (Î³Î¹Î± ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒ missing values)\n",
    "# - Memory usage\n",
    "print(cancer.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š Statistical Summary:\")\n",
    "print()\n",
    "\n",
    "# describe() Ï€Î±ÏÎ­Ï‡ÎµÎ¹ descriptive statistics:\n",
    "# - count: Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ non-null values\n",
    "# - mean: Î¼Î­ÏƒÎ¿Ï‚ ÏŒÏÎ¿Ï‚\n",
    "# - std: standard deviation (Î´Î¹Î±ÏƒÏ€Î¿ÏÎ¬)\n",
    "# - min: ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î· Ï„Î¹Î¼Î®\n",
    "# - 25%, 50%, 75%: quartiles (percentiles)\n",
    "# - max: Î¼Î­Î³Î¹ÏƒÏ„Î· Ï„Î¹Î¼Î®\n",
    "# Î£Î—ÎœÎ•Î™Î©Î£Î—: describe() Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ Î¼ÏŒÎ½Î¿ numeric columns by default\n",
    "print(cancer.describe())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# MISSING VALUES CHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nâ“ Missing Values:\")\n",
    "print()\n",
    "\n",
    "# isnull() ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ boolean DataFrame (True ÏŒÏ€Î¿Ï… Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ NaN)\n",
    "# sum() Î¼ÎµÏ„ÏÎ¬ÎµÎ¹ Ï„Î± True values Î±Î½Î¬ column\n",
    "missing = cancer.isnull().sum()\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î¼ÏŒÎ½Î¿ Ï„Ï‰Î½ columns Î¼Îµ missing values\n",
    "# Î‘Î½ missing.sum() == 0, ÎµÎ¼Ï†Î±Î½Î¯Î¶ÎµÎ¹ success message\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])  # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Î¼ÏŒÎ½Î¿ columns Î¼Îµ missing values\n",
    "    print(f\"\\nâš ï¸ Total missing values: {missing.sum()}\")\n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# TARGET DISTRIBUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ¯ Target Distribution:\")\n",
    "print()\n",
    "\n",
    "# value_counts() Î¼ÎµÏ„ÏÎ¬ÎµÎ¹ Ï„Î·Î½ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· ÎºÎ¬Î¸Îµ unique value\n",
    "# Î§ÏÎ®ÏƒÎ¹Î¼Î¿ Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿ class balance\n",
    "print(\"Counts per class:\")\n",
    "print(cancer['diagnosis'].value_counts())\n",
    "\n",
    "# normalize=True ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ proportions (0-1) Î±Î½Ï„Î¯ Î³Î¹Î± counts\n",
    "# Î Î¿Î»Î»Î±Ï€Î»Î±ÏƒÎ¹Î¬Î¶Î¿Ï…Î¼Îµ ÎµÏ€Î¯ 100 Î³Î¹Î± Ï€Î¿ÏƒÎ¿ÏƒÏ„Î¬\n",
    "print(\"\\nPercentages per class:\")\n",
    "class_distribution = cancer['diagnosis'].value_counts(normalize=True) * 100\n",
    "print(class_distribution)\n",
    "\n",
    "# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± class imbalance\n",
    "# Ideally Î¸Î­Î»Î¿Ï…Î¼Îµ balanced classes (Ï€ÎµÏÎ¯Ï€Î¿Ï… 50%-50%)\n",
    "# Î‘Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î¼ÎµÎ³Î¬Î»Î¿ imbalance (Ï€.Ï‡. 90%-10%), Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹ ÎµÎ¹Î´Î¹ÎºÎ­Ï‚ Ï„ÎµÏ‡Î½Î¹ÎºÎ­Ï‚\n",
    "benign_pct = class_distribution['B']\n",
    "malignant_pct = class_distribution['M']\n",
    "\n",
    "if abs(benign_pct - malignant_pct) > 30:\n",
    "    print(\"\\nâš ï¸ WARNING: Significant class imbalance detected!\")\n",
    "    print(\"   Consider using stratified sampling or class weights.\")\n",
    "else:\n",
    "    print(\"\\nâœ… Classes are relatively balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visualization: Target Distribution\n",
    "\n",
    "ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î·Ï‚ ÎºÎ±Ï„Î±Î½Î¿Î¼Î®Ï‚ Ï„Ï‰Î½ classes (Benign vs Malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± figure Î¼Îµ 2 subplots Î´Î¯Ï€Î»Î±-Î´Î¯Ï€Î»Î±\n",
    "# figsize=(14, 5): Ï€Î»Î¬Ï„Î¿Ï‚ 14 inches, ÏÏˆÎ¿Ï‚ 5 inches\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# 1, 2 ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹: 1 row, 2 columns\n",
    "# axes ÎµÎ¯Î½Î±Î¹ array Î¼Îµ 2 elements: axes[0] ÎºÎ±Î¹ axes[1]\n",
    "\n",
    "# ============================================================================\n",
    "# LEFT PLOT: BAR CHART\n",
    "# ============================================================================\n",
    "\n",
    "# value_counts() ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Series Î¼Îµ counts\n",
    "# plot(kind='bar') Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ bar chart\n",
    "cancer['diagnosis'].value_counts().plot(\n",
    "    kind='bar',                          # Î¤ÏÏ€Î¿Ï‚ plot\n",
    "    ax=axes[0],                         # Subplot ÏƒÏ„Î¿ Î¿Ï€Î¿Î¯Î¿ Î¸Î± ÏƒÏ‡ÎµÎ´Î¹Î±ÏƒÏ„ÎµÎ¯\n",
    "    color=['#2ecc71', '#e74c3c']       # Î ÏÎ¬ÏƒÎ¹Î½Î¿ Î³Î¹Î± Benign, ÎšÏŒÎºÎºÎ¹Î½Î¿ Î³Î¹Î± Malignant\n",
    ")\n",
    "\n",
    "# Î¤Î¯Ï„Î»Î¿Ï‚ Ï„Î¿Ï… subplot\n",
    "axes[0].set_title('Diagnosis Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Î•Ï„Î¹ÎºÎ­Ï„ÎµÏ‚ Î±Î¾ÏŒÎ½Ï‰Î½\n",
    "axes[0].set_xlabel('Diagnosis (B=Benign, M=Malignant)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Î‘Î»Î»Î±Î³Î® Ï„Ï‰Î½ x-tick labels ÏƒÎµ Ï€Î¹Î¿ descriptive\n",
    "# rotation=0 ÎºÏÎ±Ï„Î¬ÎµÎ¹ Ï„Î¿ text Î¿ÏÎ¹Î¶ÏŒÎ½Ï„Î¹Î± (default ÎµÎ¯Î½Î±Î¹ 90 Î³Î¹Î± bar plots)\n",
    "axes[0].set_xticklabels(['Benign', 'Malignant'], rotation=0)\n",
    "\n",
    "# ============================================================================\n",
    "# RIGHT PLOT: PIE CHART\n",
    "# ============================================================================\n",
    "\n",
    "# Pie chart Î³Î¹Î± visualization Ï„Ï‰Î½ proportions\n",
    "cancer['diagnosis'].value_counts().plot(\n",
    "    kind='pie',                         # Î¤ÏÏ€Î¿Ï‚ plot\n",
    "    ax=axes[1],                        # Subplot ÏƒÏ„Î¿ Î¿Ï€Î¿Î¯Î¿ Î¸Î± ÏƒÏ‡ÎµÎ´Î¹Î±ÏƒÏ„ÎµÎ¯\n",
    "    autopct='%1.1f%%',                 # Format: Ï€Î¿ÏƒÎ¿ÏƒÏ„Î¬ Î¼Îµ 1 Î´ÎµÎºÎ±Î´Î¹ÎºÏŒ ÏˆÎ·Ï†Î¯Î¿\n",
    "    colors=['#2ecc71', '#e74c3c'],     # ÎŠÎ´Î¹Î± Ï‡ÏÏÎ¼Î±Ï„Î± Î¼Îµ Ï„Î¿ bar chart\n",
    "    startangle=90                       # ÎÎµÎºÎ¹Î½Î¬ÎµÎ¹ Î±Ï€ÏŒ Ï„Î± 90Â° (top)\n",
    ")\n",
    "\n",
    "# Î¤Î¯Ï„Î»Î¿Ï‚\n",
    "axes[1].set_title('Diagnosis Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Ï„Î¿Ï… ylabel (Î´ÎµÎ½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ ÏƒÎµ pie chart)\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "# ============================================================================\n",
    "# FINALIZE\n",
    "# ============================================================================\n",
    "\n",
    "# tight_layout() Ï€ÏÎ¿ÏƒÎ±ÏÎ¼ÏŒÎ¶ÎµÎ¹ Ï„Î± subplots Î³Î¹Î± Î½Î± Î¼Î·Î½ overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î¿Ï… plot\n",
    "plt.show()\n",
    "\n",
    "# Î£Î—ÎœÎ•Î™Î©Î£Î—: Î£Ï„Î¿ Jupyter, Ï„Î¿ plt.show() ÎµÎ¯Î½Î±Î¹ optional\n",
    "# Î¤Î¿ plot ÎµÎ¼Ï†Î±Î½Î¯Î¶ÎµÏ„Î±Î¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± ÏƒÏ„Î¿ cell output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Step 4: Data Preprocessing\n",
    "\n",
    "ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎºÎ±Î¹ Ï€ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± machine learning.\n",
    "\n",
    "**Î•Î½Î­ÏÎ³ÎµÎ¹ÎµÏ‚**:\n",
    "1. Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Î¬Ï‡ÏÎ·ÏƒÏ„Ï‰Î½ columns (id, Unnamed:32)\n",
    "2. Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ features (X) ÎºÎ±Î¹ target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REMOVE UNNECESSARY COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "# Î— ÏƒÏ„Î®Î»Î· 'id' ÎµÎ¯Î½Î±Î¹ Î±Ï€Î»Î¬ identifier - Î´ÎµÎ½ Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ predictive information\n",
    "# Î— ÏƒÏ„Î®Î»Î· 'Unnamed: 32' ÎµÎ¯Î½Î±Î¹ artifact Î±Ï€ÏŒ Ï„Î¿ CSV (empty column)\n",
    "\n",
    "# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î· ÏƒÏ„Î®Î»Î· 'Unnamed: 32' Ï€ÏÎ¹Î½ Ï„Î·Î½ Î±Ï†Î±Î¹ÏÎ­ÏƒÎ¿Ï…Î¼Îµ\n",
    "# Î‘Ï…Ï„ÏŒ ÎºÎ¬Î½ÎµÎ¹ Ï„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ± Ï€Î¹Î¿ robust\n",
    "if 'Unnamed: 32' in cancer.columns:\n",
    "    columns_to_drop = ['id', 'Unnamed: 32']\n",
    "else:\n",
    "    columns_to_drop = ['id']\n",
    "\n",
    "# drop() Î±Ï†Î±Î¹ÏÎµÎ¯ Ï„Î¹Ï‚ ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ ÏƒÏ„Î®Î»ÎµÏ‚\n",
    "# axis=1 ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ drop columns (axis=0 Î¸Î± Î­ÎºÎ±Î½Îµ drop rows)\n",
    "# Î¤Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÏ„Î±Î¹ ÏƒÎµ Î½Î­Î¿ DataFrame\n",
    "cancer_clean = cancer.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"âœ… Dropped columns: {columns_to_drop}\")\n",
    "print(f\"   New shape: {cancer_clean.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINE FEATURES (X) AND TARGET (y)\n",
    "# ============================================================================\n",
    "\n",
    "# X: Feature matrix (ÏŒÎ»ÎµÏ‚ Î¿Î¹ ÏƒÏ„Î®Î»ÎµÏ‚ ÎµÎºÏ„ÏŒÏ‚ Î±Ï€ÏŒ Ï„Î¿ target)\n",
    "# Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ drop() Î³Î¹Î± Î½Î± Î±Ï†Î±Î¹ÏÎ­ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ 'diagnosis'\n",
    "X = cancer_clean.drop('diagnosis', axis=1)\n",
    "\n",
    "# y: Target vector (Î¼ÏŒÎ½Î¿ Î· ÏƒÏ„Î®Î»Î· 'diagnosis')\n",
    "# Î‘Ï…Ï„ÏŒ Ï€Î¿Ï… Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎ¿Ï…Î¼Îµ\n",
    "y = cancer_clean['diagnosis']\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nâœ… Features shape: {X.shape}\")\n",
    "print(f\"   Samples: {X.shape[0]}\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\nâœ… Target shape: {y.shape}\")\n",
    "print(f\"   Samples: {y.shape[0]}\")\n",
    "\n",
    "# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ X ÎºÎ±Î¹ y Î­Ï‡Î¿Ï…Î½ Ï„Î¿Î½ Î¯Î´Î¹Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ samples\n",
    "assert X.shape[0] == y.shape[0], \"X and y must have same number of samples!\"\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ feature names\n",
    "print(f\"\\nğŸ“‹ Feature names ({len(X.columns)} total):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· ÏƒÎµ ÏƒÏ„Î®Î»ÎµÏ‚ Ï„Ï‰Î½ 3 Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· readability\n",
    "    print(f\"{i:2d}. {col:25s}\", end=\"  \" if i % 3 != 0 else \"\\n\")\n",
    "    \n",
    "# Î‘Î½ Î¿ Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ÏƒÎ¹Î¿ Ï„Î¿Ï… 3, Ï€ÏÏŒÏƒÎ¸ÎµÏƒÎµ newline\n",
    "if len(X.columns) % 3 != 0:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”— Feature Correlation Analysis\n",
    "\n",
    "Î‘Î½Î¬Î»Ï…ÏƒÎ· Ï„Ï‰Î½ correlations Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ features.\n",
    "\n",
    "**Î“Î¹Î±Ï„Î¯ ÎµÎ¯Î½Î±Î¹ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ**:\n",
    "- Highly correlated features Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€ÏÎ¿ÎºÎ±Î»Î¿ÏÎ½ multicollinearity\n",
    "- ÎœÏ€Î¿ÏÎµÎ¯ Î½Î± Î±Ï†Î±Î¹ÏÎ­ÏƒÎ¿Ï…Î¼Îµ redundant features\n",
    "- Î’Î¿Î·Î¸Î¬ÎµÎ¹ ÏƒÏ„Î·Î½ ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Ï„Ï‰Î½ relationships ÏƒÏ„Î± data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPUTE CORRELATION MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "# corr() Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Pearson correlation coefficient (-1 to 1)\n",
    "# -1: perfect negative correlation\n",
    "#  0: no correlation\n",
    "# +1: perfect positive correlation\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "print(f\"Correlation matrix shape: {correlation_matrix.shape}\")\n",
    "print(f\"   {correlation_matrix.shape[0]} features x {correlation_matrix.shape[1]} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE CORRELATION HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î¼ÎµÎ³Î¬Î»Î¿Ï… figure Î³Î¹Î± Î½Î± Ï†Î±Î¯Î½Î¿Î½Ï„Î±Î¹ ÏŒÎ»Î± Ï„Î± features\n",
    "plt.figure(figsize=(16, 14))\n",
    "\n",
    "# seaborn heatmap Î³Î¹Î± Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î·Ï‚ correlation matrix\n",
    "sns.heatmap(\n",
    "    correlation_matrix,          # Î¤Î± data\n",
    "    annot=False,                 # ÎœÎ·Î½ Î´ÎµÎ¯Ï‡Î½ÎµÎ¹Ï‚ Ï„Î± numbers (Î¸Î± ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï crowded)\n",
    "    cmap='coolwarm',            # Colormap: Î¼Ï€Î»Îµ (negative) -> ÎºÏŒÎºÎºÎ¹Î½Î¿ (positive)\n",
    "    center=0,                    # Î¤Î¿ 0 Î½Î± ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î¿ ÎºÎ­Î½Ï„ÏÎ¿ Ï„Î¿Ï… colormap (Î»ÎµÏ…ÎºÏŒ)\n",
    "    square=True,                 # Î¤ÎµÏ„ÏÎ¬Î³Ï‰Î½Î± cells\n",
    "    linewidths=0.5,             # Î›ÎµÏ€Ï„Î­Ï‚ Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ cells\n",
    "    cbar_kws={\"shrink\": 0.8}    # ÎœÎ¹ÎºÏÏŒÏ„ÎµÏÎ¿ colorbar (80% Ï„Î¿Ï… default)\n",
    ")\n",
    "\n",
    "# Î¤Î¯Ï„Î»Î¿Ï‚ Î¼Îµ extra padding\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î® layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ·\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# FIND HIGHLY CORRELATED FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "# Î•ÏÏÎµÏƒÎ· pairs Î¼Îµ |correlation| > 0.9\n",
    "# np.where() ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î¹Ï‚ Î¸Î­ÏƒÎµÎ¹Ï‚ ÏŒÏ€Î¿Ï… Î· ÏƒÏ…Î½Î¸Î®ÎºÎ· ÎµÎ¯Î½Î±Î¹ True\n",
    "high_corr = np.where(np.abs(correlation_matrix) > 0.9)\n",
    "\n",
    "# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î»Î¯ÏƒÏ„Î±Ï‚ Î¼Îµ (feature1, feature2, correlation) tuples\n",
    "# Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î±:\n",
    "# - x != y: Î±Ï€Î¿ÎºÎ»ÎµÎ¯ÎµÎ¹ Ï„Î·Î½ auto-correlation (ÎºÎ¬Î¸Îµ feature Î¼Îµ Ï„Î¿Î½ ÎµÎ±Ï…Ï„ÏŒ Ï„Î¿Ï…)\n",
    "# - x < y: Î±Ï€Î¿ÎºÎ»ÎµÎ¯ÎµÎ¹ duplicates (Ï€.Ï‡. (A,B) ÎºÎ±Î¹ (B,A))\n",
    "high_corr_list = [\n",
    "    (\n",
    "        correlation_matrix.index[x],      # ÎŒÎ½Î¿Î¼Î± 1Î¿Ï… feature\n",
    "        correlation_matrix.columns[y],    # ÎŒÎ½Î¿Î¼Î± 2Î¿Ï… feature\n",
    "        correlation_matrix.iloc[x, y]     # Correlation value\n",
    "    ) \n",
    "    for x, y in zip(*high_corr)          # Iterate Î¼Î­ÏƒÎ± Î±Ï€ÏŒ Ï„Î¹Ï‚ Î¸Î­ÏƒÎµÎ¹Ï‚\n",
    "    if x != y and x < y                  # Î¦Î¯Î»Ï„ÏÎ±\n",
    "]\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½\n",
    "print(f\"\\nğŸ”— Highly Correlated Feature Pairs (|correlation| > 0.9):\")\n",
    "print(f\"   Found {len(high_corr_list)} pairs\\n\")\n",
    "\n",
    "if len(high_corr_list) > 0:\n",
    "    print(\"   Feature 1                   Feature 2                   Correlation\")\n",
    "    print(\"   \" + \"=\"*70)\n",
    "    \n",
    "    # Sort by absolute correlation (descending)\n",
    "    high_corr_list.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Ï€ÏÏÏ„Ï‰Î½ 10 (Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Ï„ÏŒÏƒÎ±)\n",
    "    for feat1, feat2, corr in high_corr_list[:10]:\n",
    "        print(f\"   {feat1:25s}  {feat2:25s}  {corr:6.3f}\")\n",
    "    \n",
    "    if len(high_corr_list) > 10:\n",
    "        print(f\"   ... and {len(high_corr_list) - 10} more pairs\")\n",
    "    \n",
    "    print(\"\\n   ğŸ’¡ Note: Highly correlated features may be redundant.\")\n",
    "    print(\"      Consider feature selection to remove one of each pair.\")\n",
    "else:\n",
    "    print(\"   âœ… No highly correlated pairs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ Step 5: Train-Test Split\n",
    "\n",
    "### âš ï¸ ÎšÎ¡Î™Î£Î™ÎœÎŸ Î’Î—ÎœÎ‘ - Î£Ï‰ÏƒÏ„Î® ÏƒÎµÎ¹ÏÎ¬ Î³Î¹Î± Î±Ï€Î¿Ï†Ï…Î³Î® Data Leakage!\n",
    "\n",
    "**Î£Î©Î£Î¤Î— Î£Î•Î™Î¡Î‘**:\n",
    "1. Train-Test Split Î Î¡Î©Î¤Î‘\n",
    "2. Feature Scaling/Preprocessing ÎœÎ•Î¤Î‘ (fit ÏƒÏ„Î¿ train, transform ÏƒÎµ test)\n",
    "\n",
    "**Î›Î‘Î˜ÎŸÎ£ Î£Î•Î™Î¡Î‘** (Data Leakage):\n",
    "1. âŒ Feature Scaling ÏƒÎµ ÎŸÎ›Î‘ Ï„Î± data\n",
    "2. âŒ Train-Test Split Î¼ÎµÏ„Î¬\n",
    "\n",
    "**Î“Î¹Î±Ï„Î¯**:\n",
    "- Î¤Î¿ test set Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ \"unseen data\"\n",
    "- Î‘Î½ ÎºÎ¬Î½Î¿Ï…Î¼Îµ scaling Ï€ÏÏÏ„Î±, Ï„Î¿ scaler \"Î¼Î±Î¸Î±Î¯Î½ÎµÎ¹\" Î±Ï€ÏŒ Ï„Î¿ test set\n",
    "- Î‘Ï…Ï„ÏŒ Î´Î¯Î½ÎµÎ¹ unrealistic performance estimates (overly optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN-TEST SPLIT (CORRECT WAY - NO DATA LEAKAGE!)\n",
    "# ============================================================================\n",
    "\n",
    "# train_test_split() Ï‡Ï‰ÏÎ¯Î¶ÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÎµ training ÎºÎ±Î¹ test sets\n",
    "# Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ 4 objects: X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                      # Features (DataFrame Î¼Îµ 30 columns)\n",
    "    y,                      # Target (Series Î¼Îµ diagnoses)\n",
    "    test_size=0.3,         # 30% Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± testing (171/569)\n",
    "                            # Î£Ï…Î½Î·Î¸Î¹ÏƒÎ¼Î­Î½Î±: 0.2 (80/20) Î® 0.3 (70/30)\n",
    "    random_state=42,       # Seed Î³Î¹Î± reproducibility\n",
    "                            # ÎŸ Î¯Î´Î¹Î¿Ï‚ random_state â†’ Î¯Î´Î¹Î¿ split ÎºÎ¬Î¸Îµ Ï†Î¿ÏÎ¬\n",
    "                            # 42 ÎµÎ¯Î½Î±Î¹ convention (Î±Ï€ÏŒ Hitchhiker's Guide to Galaxy)\n",
    "    stratify=y             # Î£Î—ÎœÎ‘ÎÎ¤Î™ÎšÎŸ: Î”Î¹Î±Ï„Î·ÏÎµÎ¯ Ï„Î¿ class distribution\n",
    "                            # Î‘Î½ train Î­Ï‡ÎµÎ¹ 63% B / 37% M, Ï„Î¿ test Î¸Î± Î­Ï‡ÎµÎ¹ Ï„Î¿ Î¯Î´Î¹Î¿\n",
    "                            # Î•Î¹Î´Î¹ÎºÎ¬ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ Î³Î¹Î± imbalanced datasets\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"âœ… Train-Test Split completed successfully!\")\n",
    "print()\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· shapes\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"   Samples: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "print()\n",
    "\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"   Samples: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Features: {X_test.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY STRATIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ Ï„Î¿ stratify Î»ÎµÎ¹Ï„Î¿ÏÏÎ³Î·ÏƒÎµ ÏƒÏ‰ÏƒÏ„Î¬\n",
    "# Î¤Î± Ï€Î¿ÏƒÎ¿ÏƒÏ„Î¬ ÏƒÏ„Î¿ train/test Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ Ï€Î±ÏÏŒÎ¼Î¿Î¹Î± Î¼Îµ Ï„Î¿ original dataset\n",
    "\n",
    "print(\"ğŸ¯ Class Distribution:\")\n",
    "print()\n",
    "\n",
    "# Original distribution\n",
    "original_dist = y.value_counts(normalize=True) * 100\n",
    "print(\"Original dataset:\")\n",
    "print(f\"   Benign (B):    {original_dist['B']:.1f}%\")\n",
    "print(f\"   Malignant (M): {original_dist['M']:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Training distribution\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "print(\"Training set:\")\n",
    "print(f\"   Benign (B):    {train_dist['B']:.1f}%\")\n",
    "print(f\"   Malignant (M): {train_dist['M']:.1f}%\")\n",
    "print(f\"   Total samples: {len(y_train)}\")\n",
    "print()\n",
    "\n",
    "# Test distribution\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "print(\"Test set:\")\n",
    "print(f\"   Benign (B):    {test_dist['B']:.1f}%\")\n",
    "print(f\"   Malignant (M): {test_dist['M']:.1f}%\")\n",
    "print(f\"   Total samples: {len(y_test)}\")\n",
    "print()\n",
    "\n",
    "# Î•Ï€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ· ÏŒÏ„Î¹ Î¿Î¹ ÎºÎ±Ï„Î±Î½Î¿Î¼Î­Ï‚ ÎµÎ¯Î½Î±Î¹ Ï€Î±ÏÏŒÎ¼Î¿Î¹ÎµÏ‚ (Î´Î¹Î±Ï†Î¿ÏÎ¬ < 2%)\n",
    "diff_train = abs(train_dist['B'] - original_dist['B'])\n",
    "diff_test = abs(test_dist['B'] - original_dist['B'])\n",
    "\n",
    "if diff_train < 2 and diff_test < 2:\n",
    "    print(\"âœ… Stratification successful - distributions are similar!\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: Distributions differ - stratification may have failed.\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTANT REMINDERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš ï¸ IMPORTANT: Feature Scaling/Preprocessing Rules\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… DO: Split first, then scale\")\n",
    "print(\"   scaler.fit(X_train)              # Learn from training data only\")\n",
    "print(\"   X_train_scaled = scaler.transform(X_train)\")\n",
    "print(\"   X_test_scaled = scaler.transform(X_test)   # Apply same transformation\")\n",
    "print()\n",
    "print(\"âŒ DON'T: Scale first, then split (DATA LEAKAGE!)\")\n",
    "print(\"   X_scaled = scaler.fit_transform(X)        # Wrong - uses test data!\")\n",
    "print(\"   X_train, X_test = train_test_split(...)   # Too late\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ Step 6: Feature Scaling\n",
    "\n",
    "### StandardScaler - ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Ï‰Î½ Features\n",
    "\n",
    "**Î¤Î¹ ÎºÎ¬Î½ÎµÎ¹**:\n",
    "- ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ ÎºÎ¬Î¸Îµ feature ÏÏƒÏ„Îµ: mean = 0, std = 1\n",
    "- Î¤ÏÏ€Î¿Ï‚: z = (x - Î¼) / Ïƒ\n",
    "\n",
    "**Î“Î¹Î±Ï„Î¯ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹**:\n",
    "- Features Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ ÎºÎ»Î¯Î¼Î±ÎºÎµÏ‚ (Ï€.Ï‡. 0-1 vs 0-1000)\n",
    "- Î‘Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Î¹ Ï€Î¿Ï… Î²Î±ÏƒÎ¯Î¶Î¿Î½Ï„Î±Î¹ ÏƒÎµ distances (SVM, KNN) ÎµÎ¯Î½Î±Î¹ sensitive ÏƒÏ„Î·Î½ ÎºÎ»Î¯Î¼Î±ÎºÎ±\n",
    "- Logistic Regression ÎºÎ±Î¹ Neural Networks converge Ï„Î±Ï‡ÏÏ„ÎµÏÎ±\n",
    "\n",
    "**Î ÏŒÏ„Îµ Î”Î•Î Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹**:\n",
    "- Tree-based algorithms (Random Forest, Decision Trees) Î´ÎµÎ½ ÎµÏ€Î·ÏÎµÎ¬Î¶Î¿Î½Ï„Î±Î¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE SCALER\n",
    "# ============================================================================\n",
    "\n",
    "# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± StandardScaler object\n",
    "# Î‘Ï…Ï„ÏŒ Î”Î•Î ÎºÎ¬Î½ÎµÎ¹ Î±ÎºÏŒÎ¼Î± scaling - Î±Ï€Î»Î¬ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï„Î¿ object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ­Ï‚:\n",
    "# - MinMaxScaler(): scales to [0, 1] range\n",
    "# - RobustScaler(): uses median & IQR (robust to outliers)\n",
    "# - Normalizer(): scales each sample to unit norm\n",
    "\n",
    "print(\"StandardScaler initialized\")\n",
    "print(f\"   Formula: z = (x - mean) / std\")\n",
    "print(f\"   Result: mean=0, std=1 for each feature\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# FIT ON TRAINING DATA ONLY (NO DATA LEAKAGE!)\n",
    "# ============================================================================\n",
    "\n",
    "# fit() Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ mean ÎºÎ±Î¹ std Î‘Î ÎŸ Î¤ÎŸ TRAINING SET ÎœÎŸÎÎŸ\n",
    "# Î‘Ï…Ï„Î¬ Ï„Î± statistics Î±Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿ scaler object\n",
    "# âš ï¸ CRITICAL: Î”ÎµÎ½ Ï€ÏÎ­Ï€ÎµÎ¹ Î ÎŸÎ¤Î• Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ fit ÏƒÏ„Î¿ test set!\n",
    "scaler.fit(X_train)\n",
    "\n",
    "print(\"âœ… Scaler fitted on training data\")\n",
    "print(f\"   Learned statistics from {X_train.shape[0]} samples\")\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î¼ÎµÏÎ¹ÎºÏÎ½ examples Ï„Ï‰Î½ learned statistics\n",
    "print(f\"\\n   Example learned means (first 5 features):\")\n",
    "for i, (feat, mean) in enumerate(zip(X.columns[:5], scaler.mean_[:5])):\n",
    "    print(f\"      {feat:25s}: {mean:10.4f}\")\n",
    "\n",
    "print(f\"\\n   Example learned std devs (first 5 features):\")\n",
    "for i, (feat, std) in enumerate(zip(X.columns[:5], scaler.scale_[:5])):\n",
    "    print(f\"      {feat:25s}: {std:10.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORM TRAINING DATA\n",
    "# ============================================================================\n",
    "\n",
    "# transform() ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÎ¹ Ï„Î·Î½ ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î± learned statistics\n",
    "# Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ numpy array\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "print(f\"\\nâœ… Training data transformed\")\n",
    "print(f\"   Shape: {X_train_scaled.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORM TEST DATA (USING TRAINING STATISTICS!)\n",
    "# ============================================================================\n",
    "\n",
    "# âš ï¸ CRITICAL: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ transform(), ÎŸÎ§Î™ fit_transform()!\n",
    "# Î‘Ï…Ï„ÏŒ ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÎ¹ Ï„Î± Î™Î”Î™Î‘ statistics Ï€Î¿Ï… Î¼Î¬Î¸Î±Î¼Îµ Î±Ï€ÏŒ Ï„Î¿ training set\n",
    "# Î‘Î½ ÎºÎ¬Î½Î±Î¼Îµ fit_transform(), Î¸Î± Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶Î±Î¼Îµ ÎÎ•Î‘ statistics Î±Ï€ÏŒ Ï„Î¿ test set\n",
    "# (data leakage!)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"âœ… Test data transformed (using training statistics)\")\n",
    "print(f\"   Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONVERT BACK TO DATAFRAMES (OPTIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# sklearn ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ numpy arrays, Î±Î»Î»Î¬ DataFrames ÎµÎ¯Î½Î±Î¹ Ï€Î¹Î¿ readable\n",
    "# Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± original column names ÎºÎ±Î¹ indices\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    X_train_scaled,              # Î¤Î± scaled data (numpy array)\n",
    "    columns=X.columns,           # Î¤Î± original column names\n",
    "    index=X_train.index          # Î¤Î± original row indices\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    X_test_scaled,\n",
    "    columns=X.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Converted to DataFrames for better readability\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY SCALING\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nğŸ” Verification of Scaling:\")\n",
    "print()\n",
    "\n",
    "# Training set should have mean â‰ˆ 0, std â‰ˆ 1\n",
    "train_mean = X_train_scaled.mean().mean()  # Mean of all feature means\n",
    "train_std = X_train_scaled.std().mean()    # Mean of all feature stds\n",
    "\n",
    "print(f\"Training set (should be close to 0 and 1):\")\n",
    "print(f\"   Average mean: {train_mean:.6f}  (expected: ~0.000000)\")\n",
    "print(f\"   Average std:  {train_std:.6f}  (expected: ~1.000000)\")\n",
    "\n",
    "# Test set will have slightly different mean/std (this is NORMAL and CORRECT!)\n",
    "test_mean = X_test_scaled.mean().mean()\n",
    "test_std = X_test_scaled.std().mean()\n",
    "\n",
    "print(f\"\\nTest set (will differ slightly - this is CORRECT!):\")\n",
    "print(f\"   Average mean: {test_mean:.6f}\")\n",
    "print(f\"   Average std:  {test_std:.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Note: Test set mean/std differ because we used TRAINING statistics.\")\n",
    "print(f\"   This is correct and prevents data leakage!\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARE: BEFORE vs AFTER SCALING\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nğŸ“Š Comparison: Before vs After Scaling (first feature)\")\n",
    "print()\n",
    "\n",
    "first_feature = X.columns[0]\n",
    "\n",
    "print(f\"Feature: {first_feature}\")\n",
    "print(f\"   Before scaling:\")\n",
    "print(f\"      Mean: {X_train[first_feature].mean():10.4f}\")\n",
    "print(f\"      Std:  {X_train[first_feature].std():10.4f}\")\n",
    "print(f\"      Min:  {X_train[first_feature].min():10.4f}\")\n",
    "print(f\"      Max:  {X_train[first_feature].max():10.4f}\")\n",
    "print()\n",
    "print(f\"   After scaling:\")\n",
    "print(f\"      Mean: {X_train_scaled[first_feature].mean():10.4f}\")\n",
    "print(f\"      Std:  {X_train_scaled[first_feature].std():10.4f}\")\n",
    "print(f\"      Min:  {X_train_scaled[first_feature].min():10.4f}\")\n",
    "print(f\"      Max:  {X_train_scaled[first_feature].max():10.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DISPLAY SAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nğŸ“‹ Scaled Training Data Sample (first 3 rows, first 5 columns):\")\n",
    "print(X_train_scaled.iloc[:3, :5])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… FEATURE SCALING COMPLETED CORRECTLY (NO DATA LEAKAGE!)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 7: Feature Selection\n",
    "\n",
    "Î•Ï€Î¹Î»Î¿Î³Î® Ï„Ï‰Î½ Ï€Î¹Î¿ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏÎ½ features Î³Î¹Î± Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿.\n",
    "\n",
    "**Î“Î¹Î±Ï„Î¯ Feature Selection**:\n",
    "- ÎœÎµÎ¹ÏÎ½ÎµÎ¹ Ï„Î¿ dimensionality (curse of dimensionality)\n",
    "- Î’ÎµÎ»Ï„Î¹ÏÎ½ÎµÎ¹ Ï„Î·Î½ Ï„Î±Ï‡ÏÏ„Î·Ï„Î± training\n",
    "- ÎœÎµÎ¹ÏÎ½ÎµÎ¹ Ï„Î¿ overfitting\n",
    "- Î’ÎµÎ»Ï„Î¹ÏÎ½ÎµÎ¹ Ï„Î·Î½ interpretability\n",
    "\n",
    "**ÎœÎ­Î¸Î¿Î´Î¿Ï‚: SelectKBest Î¼Îµ ANOVA F-test**\n",
    "- Univariate statistical test\n",
    "- ÎœÎµÏ„ÏÎ¬ÎµÎ¹ Ï„Î·Î½ \"importance\" ÎºÎ¬Î¸Îµ feature Î¾ÎµÏ‡Ï‰ÏÎ¹ÏƒÏ„Î¬\n",
    "- Î“ÏÎ®Î³Î¿ÏÎ¿ ÎºÎ±Î¹ Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE SELECTKBEST\n",
    "# ============================================================================\n",
    "\n",
    "# SelectKBest ÎµÏ€Î¹Î»Î­Î³ÎµÎ¹ Ï„Î± k ÎºÎ±Î»ÏÏ„ÎµÏÎ± features Î²Î¬ÏƒÎµÎ¹ statistical test\n",
    "selector = SelectKBest(\n",
    "    score_func=f_classif,     # ANOVA F-test Î³Î¹Î± classification\n",
    "                               # Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ­Ï‚: chi2, mutual_info_classif\n",
    "    k=15                       # Î•Ï€Î¹Î»Î­Î³Î¿Ï…Î¼Îµ Ï„Î± 15 ÎºÎ±Î»ÏÏ„ÎµÏÎ± Î±Ï€ÏŒ 30\n",
    "                               # ÎœÏ€Î¿ÏÎµÎ¯Ï‚ Î½Î± Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÎµÎ¹Ï‚ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬ k (10, 15, 20)\n",
    ")\n",
    "\n",
    "print(f\"SelectKBest initialized\")\n",
    "print(f\"   Scoring function: ANOVA F-test (f_classif)\")\n",
    "print(f\"   Number of features to select: 15 out of {X_train_scaled.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# FIT AND TRANSFORM (ON TRAINING DATA)\n",
    "# ============================================================================\n",
    "\n",
    "# fit_transform():\n",
    "# 1. fit(): Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ F-scores Î³Î¹Î± ÎºÎ¬Î¸Îµ feature (Î±Ï€ÏŒ training data Î¼ÏŒÎ½Î¿!)\n",
    "# 2. transform(): ÎšÏÎ±Ï„Î¬ÎµÎ¹ Î¼ÏŒÎ½Î¿ Ï„Î± top k features\n",
    "# Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ numpy array Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î± selected features\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"âœ… Feature selection on training data\")\n",
    "print(f\"   Original features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"   Selected features: {X_train_selected.shape[1]}\")\n",
    "print(f\"   Reduction: {(1 - X_train_selected.shape[1]/X_train_scaled.shape[1])*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORM TEST DATA (USING SAME FEATURES!)\n",
    "# ============================================================================\n",
    "\n",
    "# transform() ÎºÏÎ±Ï„Î¬ÎµÎ¹ Ï„Î± Î™Î”Î™Î‘ features Ï€Î¿Ï… ÎµÏ€Î¹Î»Î­Ï‡Î¸Î·ÎºÎ±Î½ Î±Ï€ÏŒ Ï„Î¿ training set\n",
    "# ÎŸÎ§Î™ fit_transform() - Î¸Î± ÎµÏ€Î­Î»ÎµÎ³Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬ features (data leakage!)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "print(f\"âœ… Feature selection on test data (using same features)\")\n",
    "print(f\"   Original features: {X_test_scaled.shape[1]}\")\n",
    "print(f\"   Selected features: {X_test_selected.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# GET SELECTED FEATURE NAMES\n",
    "# ============================================================================\n",
    "\n",
    "# get_support() ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ boolean mask:\n",
    "# True Î³Î¹Î± selected features, False Î³Î¹Î± discarded\n",
    "selected_mask = selector.get_support()\n",
    "\n",
    "# Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ mask Î³Î¹Î± Î½Î± Ï€Î¬ÏÎ¿Ï…Î¼Îµ Ï„Î± Î¿Î½ÏŒÎ¼Î±Ï„Î±\n",
    "selected_features = X.columns[selected_mask].tolist()\n",
    "\n",
    "print(f\"âœ… Selected feature names:\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE SCORES\n",
    "# ============================================================================\n",
    "\n",
    "# scores_ Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¿ F-score Î³Î¹Î± ÎºÎ¬Î¸Îµ feature\n",
    "# Î¥ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿ score = Ï€Î¹Î¿ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ feature\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,           # ÎŒÎ»Î± Ï„Î± feature names\n",
    "    'Score': selector.scores_,      # Î¤Î± F-scores\n",
    "    'Selected': selected_mask       # Boolean: selected Î® ÏŒÏ‡Î¹\n",
    "}).sort_values('Score', ascending=False)  # Sort by score (descending)\n",
    "\n",
    "print(f\"ğŸ“Š Feature Importance Scores (ANOVA F-test):\")\n",
    "print()\n",
    "print(f\"   Rank  Feature                         Score      Selected\")\n",
    "print(f\"   \" + \"=\"*65)\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ features Î¼Îµ Ï„Î± scores Ï„Î¿Ï…Ï‚\n",
    "for idx, (_, row) in enumerate(feature_scores.iterrows(), 1):\n",
    "    # Î£ÏÎ¼Î²Î¿Î»Î¿ Î³Î¹Î± selected/not selected\n",
    "    symbol = \"âœ…\" if row['Selected'] else \"âŒ\"\n",
    "    print(f\"   {idx:2d}.   {row['Feature']:30s} {row['Score']:8.2f}    {symbol}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "selected_scores = feature_scores[feature_scores['Selected']]['Score']\n",
    "discarded_scores = feature_scores[~feature_scores['Selected']]['Score']\n",
    "\n",
    "print(f\"ğŸ“ˆ Score Statistics:\")\n",
    "print(f\"   Selected features:\")\n",
    "print(f\"      Mean score:  {selected_scores.mean():.2f}\")\n",
    "print(f\"      Min score:   {selected_scores.min():.2f}\")\n",
    "print(f\"      Max score:   {selected_scores.max():.2f}\")\n",
    "print()\n",
    "print(f\"   Discarded features:\")\n",
    "print(f\"      Mean score:  {discarded_scores.mean():.2f}\")\n",
    "print(f\"      Min score:   {discarded_scores.min():.2f}\")\n",
    "print(f\"      Max score:   {discarded_scores.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visualization: Feature Importance\n",
    "\n",
    "Bar plot Ï„Ï‰Î½ top 15 features Î¼Îµ Ï„Î± F-scores Ï„Î¿Ï…Ï‚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARE DATA FOR VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Î Î¬ÏÎµ Ï„Î± top 15 features (Ï„Î± selected)\n",
    "top_features = feature_scores.head(15)\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE BAR PLOT\n",
    "# ============================================================================\n",
    "\n",
    "# ÎœÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿ figure Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· readability\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Horizontal bar plot (Ï€Î¹Î¿ ÎµÏÎºÎ¿Î»Î¿ Î½Î± Î´Î¹Î±Î²Î±ÏƒÏ„Î¿ÏÎ½ Ï„Î± Î¿Î½ÏŒÎ¼Î±Ï„Î±)\n",
    "sns.barplot(\n",
    "    data=top_features,       # Î¤Î¿ DataFrame Î¼Îµ Ï„Î± data\n",
    "    y='Feature',             # Y-axis: feature names\n",
    "    x='Score',               # X-axis: F-scores\n",
    "    palette='viridis'        # Colormap: viridis (ÎºÎ¯Ï„ÏÎ¹Î½Î¿ -> Î¼Ï€Î»Îµ)\n",
    "                              # Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ­Ï‚: 'rocket', 'mako', 'flare'\n",
    ")\n",
    "\n",
    "# Î¤Î¯Ï„Î»Î¿Ï‚\n",
    "plt.title(\n",
    "    'Top 15 Most Important Features (ANOVA F-Score)',\n",
    "    fontsize=14,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "# Î•Ï„Î¹ÎºÎ­Ï„ÎµÏ‚ Î±Î¾ÏŒÎ½Ï‰Î½\n",
    "plt.xlabel('F-Score (Higher = More Important)')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "# Grid Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· readability\n",
    "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î® layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ·\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# INTERPRETATION NOTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ’¡ Interpretation:\")\n",
    "print(\"   - Higher F-score = stronger relationship with target\")\n",
    "print(\"   - F-score tests if feature means differ between classes\")\n",
    "print(\"   - Top features are most discriminative for Benign vs Malignant\")\n",
    "print(\"\\n   Note: These are univariate scores (each feature tested independently)\")\n",
    "print(\"         They don't account for feature interactions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 8: Baseline Model Comparison\n",
    "\n",
    "Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Ï€Î¿Î»Î»ÏÎ½ ML Î±Î»Î³Î¿ÏÎ¯Î¸Î¼Ï‰Î½ Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¿Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿.\n",
    "\n",
    "**ÎœÎ¿Î½Ï„Î­Î»Î± Ï€Î¿Ï… Î´Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ**:\n",
    "1. **Logistic Regression**: Linear model, fast, interpretable\n",
    "2. **Random Forest**: Ensemble of trees, handles non-linearity\n",
    "3. **SVM (RBF kernel)**: Non-linear, powerful in high dimensions\n",
    "4. **SVM (Linear kernel)**: Linear version of SVM\n",
    "5. **Gradient Boosting**: Boosting ensemble, often very accurate\n",
    "6. **K-Nearest Neighbors**: Instance-based, simple\n",
    "7. **Naive Bayes**: Probabilistic, fast, works well with small data\n",
    "\n",
    "**Evaluation Î¼Îµ Cross-Validation**:\n",
    "- 5-fold Stratified Cross-Validation\n",
    "- Î Î¹Î¿ Î±Î¾Î¹ÏŒÏ€Î¹ÏƒÏ„Î· ÎµÎºÏ„Î¯Î¼Î·ÏƒÎ· Î±Ï€ÏŒ simple train-test split\n",
    "- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ ÏŒÎ»Î± Ï„Î± training data Î³Î¹Î± validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEFINE MODELS TO COMPARE\n",
    "# ============================================================================\n",
    "\n",
    "# Dictionary Î¼Îµ model name -> model object\n",
    "# Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ default hyperparameters Ï€ÏÎ¿Ï‚ Ï„Î¿ Ï€Î±ÏÏŒÎ½\n",
    "# Î˜Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ tuning Î±ÏÎ³ÏŒÏ„ÎµÏÎ± Î³Î¹Î± Ï„Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ± Î¼Î¿Î½Ï„Î­Î»Î±\n",
    "models = {\n",
    "    # Linear model Î³Î¹Î± classification\n",
    "    # max_iter=5000: ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ iterations Î³Î¹Î± convergence\n",
    "    # random_state=42: reproducibility\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=5000,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # Ensemble of 100 decision trees\n",
    "    # n_estimators=100: Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ trees (Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± = Ï€Î¹Î¿ Î±ÏÎ³ÏŒ Î±Î»Î»Î¬ Ï€Î¹Î¿ stable)\n",
    "    # random_state=42: reproducibility\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # SVM Î¼Îµ Radial Basis Function kernel (non-linear)\n",
    "    # kernel='rbf': Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± handle non-linear relationships\n",
    "    # probability=True: enable predict_proba() (Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î³Î¹Î± ROC curve)\n",
    "    # random_state=42: reproducibility\n",
    "    'SVM (RBF)': SVC(\n",
    "        kernel='rbf',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # SVM Î¼Îµ linear kernel (faster, simpler)\n",
    "    # kernel='linear': linear decision boundary\n",
    "    # probability=True: enable predict_proba()\n",
    "    # random_state=42: reproducibility\n",
    "    'SVM (Linear)': SVC(\n",
    "        kernel='linear',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # Gradient Boosting classifier\n",
    "    # n_estimators=100: Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ boosting stages\n",
    "    # random_state=42: reproducibility\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # K-Nearest Neighbors\n",
    "    # n_neighbors=5: Ï„Î±Î¾Î¹Î½Î¿Î¼ÎµÎ¯ Î²Î¬ÏƒÎµÎ¹ Ï„Ï‰Î½ 5 Ï€Î»Î·ÏƒÎ¹Î­ÏƒÏ„ÎµÏÏ‰Î½ Î³ÎµÎ¹Ï„ÏŒÎ½Ï‰Î½\n",
    "    # Î£Ï…Î½Î·Î¸Î¹ÏƒÎ¼Î­Î½ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚: 3, 5, 7 (odd numbers Î³Î¹Î± Î±Ï€Î¿Ï†Ï…Î³Î® ties)\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(\n",
    "        n_neighbors=5\n",
    "    ),\n",
    "    \n",
    "    # Naive Bayes Î¼Îµ Gaussian assumption\n",
    "    # Î¥Ï€Î¿Î¸Î­Ï„ÎµÎ¹ ÏŒÏ„Î¹ features follow Gaussian distribution\n",
    "    # Î‘Ï€Î»ÏŒ, Î³ÏÎ®Î³Î¿ÏÎ¿, works well Î³Î¹Î± small datasets\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(f\"âœ… Defined {len(models)} models for comparison:\")\n",
    "for i, name in enumerate(models.keys(), 1):\n",
    "    print(f\"   {i}. {name}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "# Stratified K-Fold: Î´Î¹Î±Ï„Î·ÏÎµÎ¯ Ï„Î¿ class distribution ÏƒÎµ ÎºÎ¬Î¸Îµ fold\n",
    "# n_splits=5: Ï‡Ï‰ÏÎ¯Î¶ÎµÎ¹ Ï„Î± data ÏƒÎµ 5 folds\n",
    "# shuffle=True: shuffle Ï„Î± data Ï€ÏÎ¹Î½ Ï„Î¿ split\n",
    "# random_state=42: reproducibility\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Cross-validation setup:\")\n",
    "print(f\"   Method: Stratified K-Fold\")\n",
    "print(f\"   Number of folds: 5\")\n",
    "print(f\"   Each fold: ~{len(X_train)//5} samples\")\n",
    "print()\n",
    "\n",
    "# Î ÏÏ‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ 5-fold CV:\n",
    "# Split Ï„Î± data ÏƒÎµ 5 Î¼Î­ÏÎ·: [A, B, C, D, E]\n",
    "# Fold 1: Train [B,C,D,E], Validate [A]\n",
    "# Fold 2: Train [A,C,D,E], Validate [B]\n",
    "# Fold 3: Train [A,B,D,E], Validate [C]\n",
    "# Fold 4: Train [A,B,C,E], Validate [D]\n",
    "# Fold 5: Train [A,B,C,D], Validate [E]\n",
    "# â†’ 5 scores, average Ï„Î¿Ï…Ï‚ = CV score\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN AND EVALUATE EACH MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# List Î³Î¹Î± Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½\n",
    "results = []\n",
    "\n",
    "print(\"ğŸ”„ Training and evaluating models with 5-fold cross-validation...\")\n",
    "print(\"   This may take a few minutes...\\n\")\n",
    "\n",
    "# Iterate Î¼Î­ÏƒÎ± Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ Î¼Î¿Î½Ï„Î­Î»Î¿\n",
    "for name, model in models.items():\n",
    "    print(f\"ğŸ“Š {name}...\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CROSS-VALIDATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    # cross_val_score() Ï„ÏÎ­Ï‡ÎµÎ¹ k-fold CV ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ k scores\n",
    "    cv_scores = cross_val_score(\n",
    "        model,                    # Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿\n",
    "        X_train_scaled,           # Features (scaled)\n",
    "        y_train,                  # Target\n",
    "        cv=cv,                    # CV strategy (5-fold stratified)\n",
    "        scoring='accuracy'        # Metric (Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ 'f1', 'roc_auc', etc.)\n",
    "    )\n",
    "    # cv_scores ÎµÎ¯Î½Î±Î¹ array Î¼Îµ 5 values (Î­Î½Î± Î±Î½Î¬ fold)\n",
    "    \n",
    "    print(f\"   CV scores: {cv_scores}\")\n",
    "    print(f\"   CV mean: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRAIN ON FULL TRAINING SET\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Î¤ÏÏÎ± train ÏƒÏ„Î± ÎŸÎ›ÎŸÎšÎ›Î—Î¡Î‘ training data\n",
    "    # (Î³Î¹Î± final predictions ÏƒÏ„Î¿ test set)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEST SET PREDICTIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Predictions ÏƒÏ„Î¿ test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Prediction probabilities (Î³Î¹Î± ROC curve Î±ÏÎ³ÏŒÏ„ÎµÏÎ±)\n",
    "    # ÎœÏŒÎ½Î¿ Î±Î½ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹ predict_proba()\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability Î³Î¹Î± class 'M'\n",
    "    else:\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CALCULATE METRICS\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Accuracy: (TP + TN) / Total\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Precision: TP / (TP + FP) - \"Î ÏŒÏƒÎµÏ‚ Î±Ï€ÏŒ Ï„Î¹Ï‚ positive predictions Î®Ï„Î±Î½ ÏƒÏ‰ÏƒÏ„Î­Ï‚\"\n",
    "    # pos_label='M': Î¸ÎµÏ‰ÏÎ¿ÏÎ¼Îµ 'M' (Malignant) Ï‰Ï‚ positive class\n",
    "    precision = precision_score(y_test, y_pred, pos_label='M')\n",
    "    \n",
    "    # Recall/Sensitivity: TP / (TP + FN) - \"Î ÏŒÏƒÎ± Î±Ï€ÏŒ Ï„Î± actual positives Î²ÏÎ®ÎºÎ±Î¼Îµ\"\n",
    "    recall = recall_score(y_test, y_pred, pos_label='M')\n",
    "    \n",
    "    # F1-Score: Harmonic mean of precision and recall\n",
    "    # 2 * (precision * recall) / (precision + recall)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='M')\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STORE RESULTS\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ metrics ÏƒÎµ dictionary\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'CV Mean': cv_scores.mean(),          # Cross-validation mean\n",
    "        'CV Std': cv_scores.std(),            # Cross-validation std\n",
    "        'Test Accuracy': accuracy,             # Test set accuracy\n",
    "        'Precision': precision,                # Test set precision\n",
    "        'Recall': recall,                      # Test set recall\n",
    "        'F1-Score': f1                         # Test set F1-score\n",
    "    })\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"   Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE RESULTS DATAFRAME\n",
    "# ============================================================================\n",
    "\n",
    "# ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Î·Ï‚ results list ÏƒÎµ DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by Test Accuracy (descending)\n",
    "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "# ============================================================================\n",
    "# DISPLAY SUMMARY TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î¿Ï… DataFrame Ï‡Ï‰ÏÎ¯Ï‚ index\n",
    "print(results_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY BEST MODELS\n",
    "# ============================================================================\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_accuracy = results_df.iloc[0]['Test Accuracy']\n",
    "\n",
    "print(f\"\\nğŸ† Best Model (by Test Accuracy): {best_model_name}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Top 3 Î³Î¹Î± hyperparameter tuning Î±ÏÎ³ÏŒÏ„ÎµÏÎ±\n",
    "print(f\"\\nğŸ“ˆ Top 3 Models Î³Î¹Î± Hyperparameter Tuning:\")\n",
    "for i, row in results_df.head(3).iterrows():\n",
    "    print(f\"   {row['Model']:25s}: {row['Test Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Î¥Ï€ÏŒÎ»Î¿Î¹Ï€Î¿ notebook Î¸Î± ÏƒÏ…Î½ÎµÏ‡Î¯ÏƒÎµÎ¹ ÏƒÏ„Î¿ ÎµÏ€ÏŒÎ¼ÎµÎ½Î¿ response Î»ÏŒÎ³Ï‰ Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
